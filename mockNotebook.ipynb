{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this to write out my resume, then copying it. \n",
    "\n",
    "Would be a fun project to do this automatically, now that I have all the styling etc. for it :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daniel McCann-Sayles' Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"display: flex; justify-content: space-between; right-margin: 50px \">\n",
    "  <span>Cisco</span>\n",
    "  <span \">thing2</span>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary :p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cisco - Intern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kid Fight Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My friend Devin is interested in game design. I was talking with him about how having ‘real AI’ in games is going to allow awesome interactive stuff. I thought it would be fun to see what could be done creatively with AI\n",
    "\n",
    "As a kid I used to have arguments with friends where we would one up the other. I would have a laser, he would have a laser deflecting mirror, I would have gone back in time and weakened his mirror so it cracked, etc. \n",
    "\n",
    "I thought it would be fun to approximate some of this back and forth creativity - in a table top manner, like Pokemon.\n",
    "There were two distinct phases to this project.\n",
    "\n",
    "I'll talk a bit about each, but only show code for the second. You can check out the repository here *TODO*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First pass / Pygame era:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first started by finding a pygame game implementing chess. It made sense to just start with that chessboard. Unforunately this introduced a pattern of coupled game display and logic that lasted for a while. \n",
    "\n",
    "This was pretty fun. Lots of learning how to use pygame. Learned about pygame events to allow certain things to happen in time w/ clock. For calling chatgpt we did this asynchronously. To do this I made a new thread, which is debatably the right answer, but it works fine. We started implementing some path finding and logic for the little game pieces. \n",
    "\n",
    "I learned a fair amount about OOP as well, something I'm not super familiar with. Pygame is cool, but it also took a fair amount of boilerplate to make something simple. Though maybe this is just all game development -> not my field :).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second pass / Simulation era:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I ended up being busy for a bit, but when I came back I had a fresh perspective. I realized that this would never be fun/useful unless two people could type, which necessitates multiplayer. So I set up a simple network protocol using the socket library. \n",
    "\n",
    "This also led me to decouple the display logic and the game logic, and cleared up a lot. Instead of having all these pygame classes that handled their state and display, I have all that logic in python classes on the server. I communicate the board state through a json object, and have pygame classes on the frontend/client. You can see below the seperation of logic - the client cares about looks, and the server cares about functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Client implementation of Board\n",
    "class Board:\n",
    "    def __init__(self, size: int, x_offset, y_offset, rows: int):\n",
    "        self.width = size\n",
    "        self.height = size\n",
    "        self.x_offset = x_offset\n",
    "        self.y_offset = y_offset\n",
    "        self.square_size = size // rows\n",
    "        self.rows = rows\n",
    "        self.squares: list[Square] = self.generate_squares()\n",
    "\n",
    "    def update_board(self, board_list: list[str | PieceData]):\n",
    "        \"\"\"Update board from json data. Clear everything on squares, then re-add\"\"\"\n",
    "        if len(self.squares) != len(board_list):\n",
    "            print(\n",
    "                \"This should not happen.. mismatch btwn server and client \",\n",
    "                len(self.squares),\n",
    "                len(board_list),\n",
    "            )\n",
    "\n",
    "        for data, square in zip(board_list, self.squares):\n",
    "            square.occupying_piece = None\n",
    "            square.highlight = False\n",
    "\n",
    "            if is_piece_data(data):\n",
    "                piece = Piece(self.square_size, data)\n",
    "                square.occupying_piece = piece\n",
    "\n",
    "            elif data == \"H\":\n",
    "                square.highlight = True\n",
    "\n",
    "    def draw(self, display):\n",
    "        for square in self.squares:\n",
    "            square.draw(display)\n",
    "            \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Server implementation of Board\n",
    "class Board:\n",
    "    def __init__(self, rows: int):\n",
    "        self.rows = rows\n",
    "        self.squares: list[list[Square]] = self.generate_squares()\n",
    "        self.bases = self.make_bases()\n",
    "        self.characters: dict[str, list[Character]] = {\"black\": [], \"white\": []}\n",
    "\n",
    "    def to_json(self):\n",
    "        \"\"\"Got through the squares and serialize each of them and return it. Returns a flattened list\"\"\"\n",
    "        flat_squares = [square for columns in self.squares for square in columns]\n",
    "        json_list = [square.to_json() for square in flat_squares]\n",
    "\n",
    "        return json_list\n",
    "\n",
    "    def generate_squares(self):\n",
    "        output: list[list[Square]] = []\n",
    "        for row in range(self.rows):\n",
    "            inner_list = []\n",
    "            for column in range(self.rows):\n",
    "                inner_list.append(Square(row, column))\n",
    "\n",
    "            output.append(inner_list)\n",
    "        return output\n",
    "\n",
    "    def add_piece_safe(self, pos: tuple[int], piece: DefaultPiece):\n",
    "        \"\"\"Add the given piece to the given position on the board. Fails quietly w/ console log. Sets the pieces position to square position\"\"\"\n",
    "        square = self.get_pos(pos)\n",
    "        if not square:\n",
    "            print(\"invalid square\", pos)\n",
    "            return\n",
    "        if self.check_if_occupied(square):\n",
    "            print(\"already occupied\")\n",
    "            return\n",
    "\n",
    "        square.occupying_piece = piece\n",
    "        piece.set_pos((square.row, square.column))\n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They communicate through a simple network protocol, using sockets.\n",
    "\n",
    "`mserver.py` hosts a server which starts threaded processes as clients join. `mclient.py` initiates a new pygame (`SrverGame.py`) process that uses the Network (`network.py`) class to communicate with the server. \n",
    "\n",
    "Below are snippets from each to hopefully convey their main functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mserver\n",
    "import socket\n",
    "from _thread import start_new_thread\n",
    "from client.helpers import get_mock_board\n",
    "import json\n",
    "from server.ServerGame import ServerGame\n",
    "from address import SERVER_ADDRESS\n",
    "\n",
    "# Class to handle game logic. \n",
    "server_game = ServerGame()\n",
    "\n",
    "# Global to communicate whether players are here or not\n",
    "players = {\"black\": False, \"white\": False}\n",
    "\n",
    "def threaded_client(conn: socket.socket, color: str):\n",
    "    # Start connection by sending initial obj\n",
    "    initial = json.dumps(\n",
    "        {\"loading\": False, \"board\": server_game.board.to_json()}\n",
    "    ).encode()\n",
    "    conn.send(initial)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "           # Recieve Data ..\n",
    "\n",
    "            # If we get data saying we want a new character\n",
    "            if \"description\" in json_data:\n",
    "                server_game.create_character(color, json_data[\"description\"])\n",
    "                print(\"recieved description \", json_data[\"description\"])\n",
    "\n",
    "            # Call this every iteration. handles game logic & and other stuff\n",
    "            board_json, loading = server_game.gameloop(color)\n",
    "\n",
    "            # Return \n",
    "            conn.sendall(json.dumps({\"loading\": loading, \"board\": board_json}).encode())\n",
    "        except Exception as e:\n",
    "           break\n",
    "\n",
    "    print(\"Lost connection to \", color)\n",
    "    players[color] = False\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "\"\"\" Handle new connections\"\"\"\n",
    "while True:\n",
    "    p1 = players[\"black\"]\n",
    "    p2 = players[\"white\"]\n",
    "\n",
    "    conn, addr = s.accept()\n",
    "\n",
    "    # Add players .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mclient => ServerGame.py\n",
    "import time\n",
    "import threading\n",
    "from llm.call_gpt import generate_character_stats_multiplayer\n",
    "from server.classes.Board import Board\n",
    "from server.classes.Character import Character\n",
    "\n",
    "\n",
    "class Clock:\n",
    "    def __init__(self, interval, stopped=False):\n",
    "        \"\"\"Interval in seconds\"\"\"\n",
    "        self.interval = interval\n",
    "        self.last_time = time.time()\n",
    "        self.stopped = stopped\n",
    "\n",
    "    def check(self):\n",
    "        \"\"\"Returns true if interval has passed\"\"\"\n",
    "\n",
    "    ...\n",
    "\n",
    "\n",
    "class ServerGame:\n",
    "    def __init__(self):\n",
    "        self.main_clock = Clock(1)\n",
    "        self.sub_clock = Clock(1, stopped=True)\n",
    "\n",
    "        # Board object for representation.\n",
    "        self.board = Board(10)\n",
    "\n",
    "        # Lock prevents main check from running twice (called in two threads)\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def callback(self, response, color):\n",
    "        \"\"\"Callback is a method to have access to self - called in thread to get async GPT\"\"\"\n",
    "        self.character_to_add[color] = Character(\n",
    "            (0, 0),\n",
    "            color=color,\n",
    "            board=self.board,\n",
    "            attack_dmg=response[\"AD\"],\n",
    "            hp=response[\"HP\"],\n",
    "            move_distance=response[\"MD\"],\n",
    "        )\n",
    "\n",
    "    def create_character(self, color, description: str):\n",
    "        \"Call gpt to create a character. Currently making a new thread to do so. daemon means it will be terminated\"\n",
    "        if self.loading_character[color]:\n",
    "            return\n",
    "\n",
    "        self.loading_character[color] = True\n",
    "        description = description or \"bland default character\"\n",
    "\n",
    "        threading.Thread(\n",
    "            target=generate_character_stats_multiplayer,\n",
    "            args=(description, color, self.callback),\n",
    "            daemon=True,\n",
    "        ).start()\n",
    "\n",
    "   \n",
    "\n",
    "    def gameloop(self, color):\n",
    "        \"\"\"Runs constantly in while loop\"\"\"\n",
    "        if self.main_clock.check():\n",
    "            # Do stuff..\n",
    "\n",
    "        if self.sub_clock.check():\n",
    "            self.move_character()\n",
    "\n",
    "        # Check if we need to add a character\n",
    "        character = self.character_to_add[color]\n",
    "        if character:\n",
    "            # Add character..\n",
    "\n",
    "        # Return current board\n",
    "        return self.board.to_json(), self.loading_character[color]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original idea of this game was to explore the added creativity an LLM could bring. In this case, the idea was to have an impartial judge add or subtract to bonuses to characters depending on the existing characters on the board.\n",
    "\n",
    "A next step was to add behavior, and then to add special moves, but we did not get around to this.\n",
    "Instead we were pretty sidetracked by building out the main game engine functionality.\n",
    "Anyways heres the general prompt/schema we hoped to follow.\n",
    "\n",
    "Of some note is the Strength section. We wanted to allow the LLM judge to give bonuses to characters against other characters specifically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARACTER_PROMPT = \"\"\"\n",
    "You are a character creator. Your job is to determine the stats of a character based on these two things:\n",
    "1. A list of existing characters on the board (a list of json objects, of the format {id: <int>, description: <str>}) \n",
    "2. A description of the character passed in (a string)\n",
    "\n",
    "These stats are as follows:\n",
    "1. HP - Health points. This is not to exceed 10, or go below 1. Some examples for reference:\n",
    "    A knight in armor might have a high HP, like 6. An archer might have a low HP, like 3. A mouse might have a tiny HP, like 1.\n",
    "\n",
    "    ...\n",
    "\"\"\"\n",
    "\n",
    "API_SCHEMA = {\n",
    "    \"name\": \"character_stats\",\n",
    "    \"strict\": True,\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"HP\": {\"type\": \"integer\"},\n",
    "            \"AD\": {\"type\": \"integer\"},\n",
    "            \"MD\": {\"type\": \"integer\"},\n",
    "            \"Behavior\": {\"type\": \"integer\"},\n",
    "            \"Strength\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"against_id\": {\"type\": [\"integer\", \"null\"]},\n",
    "                    \"modifier\": {\"type\": [\"integer\", \"null\"]},\n",
    "                },\n",
    "                \"required\": [\"against_id\", \"modifier\"],\n",
    "                \"additionalProperties\": False,\n",
    "            },\n",
    "        ...\n",
    "}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection:\n",
    "\n",
    "1. Learned more about async and anyio, network protocol, and OOP w/ Python\n",
    "2. It's important to always decouple logic & display - it would've been much easier to refactor for the simulation if I had followed this practice from the beginning\n",
    "\n",
    "Next steps:\n",
    "\n",
    "In hindsight, we tried to tackle two things at the same time, and it would have been better to only tackle one. Either a =>\n",
    "1. creative response game using AI to evaluate that creativity\n",
    "2. simulator/board game, with physical pieces and a map\n",
    "\n",
    "I’d actually like to make the first one right now - instead of a physical board, using some sort of card game, Magic-like system would be much better for a prototype of on the fly, AI-judged creativity. I have some other things to do first though ..\n",
    "\n",
    "This leads to one final lesson learned, which is common in many of my projects..\n",
    "> simplify prototypes & identify measurable objectives! BEFORE starting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MACM exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During some downtime at work, I explored a Cisco research project focused on agentic AI workflows to improve performance on math problem-solving benchmarks. \n",
    "\n",
    "I optimized the code base, explored asynchronous alternatives, and reduced costs while increasing speed. I  also explored alternative methods for connecting LLM calls and evaluated their effectiveness in problem-solving.\n",
    "\n",
    "This experience sharpened my skills in prompt engineering, OpenAIs API structure, and agentic workflows (specifically debugging & dealing with overhead)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to explore chaining multiple calls to LLMs. I started with this [paper](https://arxiv.org/abs/2404.04735) as an existing benchmark. In the paper they essentially just chain a few LLM calls together and get some performance improvement on a benchmark of math problems. Their methodology essentially involves: \n",
    "\n",
    "1. Calling the LLM to examine the initial problem\n",
    "2. Repetitively asking the LLM for `conditions` that could be helpful to solve it. \n",
    "3. Doing step 2 5 times to make a pool.\n",
    "4. Asking the LLM to generate solution steps using the condition pool.\n",
    "\n",
    "They also try and give the LLMs roles, with a judge, executor, and ideator, but I don't think this matters.\n",
    "\n",
    "My goal was this project was just to learn, and mess around, using what they have as a baseline. Here is a rough timeline of what I attempted:\n",
    "\n",
    "1. First, I cloned and looked at the existing code base\n",
    "    1. I re-wrote a lot of their stuff to be modifiable, and added helper functions.\n",
    "    2. I converted most of the GPT calls to async (I didn't finish this until later). For reference, the first time I ran it on one problem it took about 7 minutes (19 in-order requests). \n",
    "    3. I cleaned up prompts, and I switched the whole thing to a cheaper form of GPT so I could run multiple times. \n",
    "    4. I also moved away from using the code editor, and I switched to pure LLM calls instead of using the OpenAI assistants. \n",
    "        > I actually spent a while messing around with this. They were using OpenAI's Assistants. OpenAI charges per code session, but once you start a code session it lasts for an hour. So if I could re-use this code session it would be cheaper. This required me to use the same conversation, so I set up some helper code to manage clearing and re-populating the message history. I finally got it working, and it only saved me ~3% of the total cost : /\n",
    "    4. I added logging so that I could start investigating what was going wrong.\n",
    "    5. By switching to just calling chat complete instead of using assistants I was able to enable schema enforcement. This allowed me to get exact types from GPT responses. This was *way* nicer than using regex and hoping. Of course, in the background all OpenAI is doing is some form of regex/heuristics + some sort of self-correction - BUT they have much more error handling than me. Why do it myself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Should I put some code here??\n",
    "# * some async stuff\n",
    "# * assistant code session stuff\n",
    "# * loggin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*At this point it would have been a good idea to re-run benchmarks. I did not do this. This led to having a hard time measuring improvement, since my only comparison was their original code, which performs better since but uses expensive GPT and drew many more samples.* \n",
    "\n",
    "2. Then I started tweaking things.  \n",
    "    1. I didn't like how they were doing the same thing over and over and averaging it. It's possible this is actually the [best way](https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt) but I didn't find it particularly interesting. Instead I wanted to try doing more correction of wrong steps - finding errors and trying to fix them directly. \n",
    "        1. Multiple checks on conditions - check if we have generated enough, and then check that. \n",
    "        2. Re-assessmemnt of solution steps + regeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE \n",
    "# * Show small additions. Consider showing diff somehow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. I noticed that the conditions as they were were not really conditions. They were basically just tyring to solve the problem. Half the time they would be steps to solve the problem. So basically just *drawing more samples* again. To do something different I wanted to do something more in order. When solving a math problem, I'd be doing different steps each time. I completely switched up the idea =>\n",
    "    1. Ask the LLM to generate a step by step solution to the problem in one pass.\n",
    "    2. Go through step by step\n",
    "    3. Verify that the step is correct. If not, regenerate it and the following steps.\n",
    "    4. Once we reach the end, try having the LLM follow those steps.\n",
    "\n",
    "*This is the graphic shown :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE - add one of the chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works okay. \n",
    "\n",
    "The code started getting a bit messy, and I got lost in terms of metrics. There were really two things that I had to manage - the overhead and building out helpers (logging, reusable functions) - and trying out different patterns of calling/prompts. It caused a fair amount of friction doing both. I see the value in tools like langchain - at least for when you're prototyping.\n",
    "\n",
    "> If I was approaching this project now I would start by defining actual metrics, and ideas for success. Exploration is fun, but its hard to make progress when you don't know where you're going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed reflections: \n",
    "\n",
    "1. This endeavor was not entirely fruitful. One big challenge is benchmarking. Every call to GPT has some element of randomness in its return. When adding multiple together, it gets very complex very quickly. Even one call can be hard since there’s no function or obvious way to calculate randomness (that I know of). It’s also kind of a nebulous problem.\n",
    "\n",
    "2. Another is how AI models are easily influenced. I imagine this is due to training that makes them agreeable to users, but when given a false idea (let’s say call #1 returned an incorrect assumption), it seems that call #2 often will not question that idea. \n",
    "\n",
    "Taking this idea a bit further, each call seems to perform worse and worse. I’m not sure why this is, though it would be interesting to read more about it. It seems impossible to do error correcting *completely* with LLM’s right now. \n",
    "\n",
    "In the end I did not achieve much noteworthy. But I understand a few things better now:\n",
    "1. python codebases\n",
    "2. python asynchronous libraries - I used anyio w/ the default configuration of asyncio backend. I like anyio.\n",
    "3. The complexities that arise in multi-LLM interactions\n",
    "4. The overhead that comes with trying to improve chained LLM performance - there are so many variables to adjust - models, prompts, chaining order, and much more. And it's not a consistent result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also had some more abstract rambling:\n",
    "\n",
    "I thought a lot about how we think, and about hiearchial forms of thought. For instance, starting at small provable concepts, using those to create higher level concepts, and so on. This is (very simplified) how we do things as humans. If I look at a complex belief I have, it’s predicated on lower and lower level ideas that have been lodged in my brain. It can be hard to change someones mind because even when a low level fact is changed, humans don't always update the higher order concepts that rest on it, or even think about that process. \n",
    "\n",
    "This leads me to something that I’ve been thinking about for a while, a way of creating a living essay. It would be cool to write a persuasive essay in a way that drew directly from the facts in it, so that when things were changed, everything downstream changed too. \n",
    "\n",
    "Of course, a lot of ideas are nebulous, but if you could lay out some amount of priors and assign some weight to each that showed how much it affected a higher level concept, then a reader could actually interact with your argument and see it change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AI Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** There are (at least) two obvious things wrong with this graph - one is that the dots overlap, meaning the last applied (gpt diff) is overrepresented. The other is that its not super helpful, except for a preliminary visualization. If I was still working on this, the next thing I would do would be sorting/clustering these data points based on how they do - e.g. these do well w/ the gpt diff, these do well with the semantic diff. \n",
    "\n",
    "\n",
    "This visualization would look cool, but would only be marginally more helpful, since it would end up with me diving back in to looking data point by data point. Ideally would be to find some way to extract similarities in the text itself, to avoid looking through each one myself. This idea of general pattern recognition reminds me of visual nn and makes me think I could train a simple model to predict how well a sample text would do on the different diff methods, and then extract the features it found. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had the opportunity to work on an internal Cisco project exploring how well an LLM email generation process was doing. A system would generate emails which are then looked over by users, optionally changed, and then sent. As a metric of how well the process was doing, the stakeholders were currently calculating a diff between the sent and generated email. \n",
    "\n",
    "I was tasked with exploring this diff, and seeing how it could be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important lesson I learned is that ‘data analysis’ is a surprisingly difficult thing to do. This motif appears later in my AI Math solving exploration as well. There are so many different ways of calculating metrics, and so many different metrics. \n",
    "\n",
    "Crucially, these metrics also depend on the audience and your intentions - the metrics you would show to an executive are completely different than those you might show to a technical stakeholder, and again different than those a customer might care about.\n",
    "\n",
    "It's fascinating. You want to create some sort of visualization out of your data that your audience can easily read. This is something I find really interesting, especially when reading AI papers. \n",
    "\n",
    "To explore the diff, I created multiple Jupyter notebooks and spent a few weeks immersing myself in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. I poured over hundreds of examples, explored different ways of both sorting & filtering, and visualizing the differences  to get impactful metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a toggleable output\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def toggleable_output(text, identifier):\n",
    "    html = f\"\"\"\n",
    "    <button onclick=\"var x = document.getElementById('toggleText_{identifier}'); x.style.display = x.style.display === 'none' ? 'block' : 'none';\">\n",
    "        Toggle {identifier}\n",
    "    </button>\n",
    "    <div id=\"toggleText_{identifier}\" style=\"display:none;\">\n",
    "        {text}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create html object to show diff visually\n",
    "def show_visual_diff_difflib(\n",
    "    old_string: str, new_string: str, wrap_column=60\n",
    "):\n",
    "    diff_html = difflib.HtmlDiff(wrapcolumn=wrap_column).make_file(old_string.split(), new_string.split(),)\n",
    "\n",
    "    # Remove legend & links\n",
    "    diff_html = re.sub(r'<table class=\"diff\" summary=\"Legends\".*?</table>', \"\", diff_html, flags=re.DOTALL)\n",
    "    diff_html = re.sub(r'</td>\\s*<td>\\s*<table border=\"\" summary=\"Links\".*?</table>\\s*</td>\\s*</tr>', \"\", diff_html, flags=re.DOTALL)\n",
    "\n",
    "    return '<style>.diff_add, .diff_chg, .diff_sub {color: black;}</style>' + diff_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow down to useful data to check out. When checking how a process affected a diff I couldn't just plot everything. It becomes a mess, as shown by the plot above\n",
    "improved_diffs = []\n",
    "for row in items: # Items is a df of calculated diff scores and text\n",
    "    new_diff = row[\"New_diff\"]\n",
    "    diff = row[\"Diff\"]\n",
    "    if new_diff - diff > .2: # I played around with this number. 0.2 usually was a good heuristic of narrowing down the data but not nitpicking\n",
    "        improved_diffs.append(row)\n",
    "\n",
    "print(f\"rows with improvements > .2: {len(improved_diffs)}, out of {len(items)}\")\n",
    "\n",
    "# For item in improved_diffs\n",
    "# Using the difflib html object + toggling to make the visualization process easier\n",
    "original_diff_text = f\"Original diff:\\n{item['Diff']}\\n\" + show_visual_diff_difflib(item[\"Generated\"], item[\"Sent\"])\n",
    "toggleable_output(original_diff_text, \"original_diff\")\n",
    "\n",
    "new_diff_text = f\" diff:\\n{item['new_diff']}\\n\" + show_visual_diff_difflib(item[\"new_diff_generated\"], item[\"new_diff_sent\"])\n",
    "toggleable_output(new_diff_text, \"new_diff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. I tried multiple methods for cleaning the data before taking the diff, including using ChatGPT and semantic similarity matching, to get a score that reflected *more accurately* how good the generation was. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the original text split into main part and trailing addition\n",
    "def check_if_trailing_addition(sent: str, generated: str):\n",
    "    testing_diff = list(\n",
    "        difflib.ndiff(\n",
    "            generated.strip().splitlines(),\n",
    "            sent.strip().splitlines(),\n",
    "        )\n",
    "    )\n",
    "    last_section_length = 0\n",
    "\n",
    "    # Find the last section of completely new lines\n",
    "    for i, line in enumerate(reversed(testing_diff)):\n",
    "        if line.startswith(\"+ \"):\n",
    "            last_section_length += 1\n",
    "        elif line.startswith(\"? \"):  # difflib metadata\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if last_section_length > 0:\n",
    "        # Split into main & trailing addition\n",
    "        sent_lines = sent.strip().splitlines()\n",
    "        main_part = \"\\n\".join(sent_lines[:-last_section_length])\n",
    "        trailing_addition = \"\\n\".join(sent_lines[-last_section_length:])\n",
    "        return trailing_addition, main_part\n",
    "\n",
    "    return False, sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to detect common signature chunks ----------\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "signature_embeddings = model.encode(known_signature_chunks, convert_to_tensor=True)\n",
    "\n",
    "# Pre-vectorize searchable text, broken down by sentence level\n",
    "def vectorize_text_by_sentences(text: str, model: SentenceTransformer):\n",
    "    sentences = text.splitlines()\n",
    "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    return sentences, sentence_embeddings\n",
    "\n",
    "def remove_signature_chunks(trailing_addition: str, signature_embeddings, model: SentenceTransformer, threshold=0.6):\n",
    "    lines, sentence_embeddings = vectorize_text_by_sentences(trailing_addition, model)\n",
    "\n",
    "     # Sliding window - to match and remove signatures\n",
    "    for start in range(len(lines)):\n",
    "        for window_size in range(1, len(lines) - start + 1):\n",
    "            # Extract the embeddings for the current window of sentences\n",
    "            window_embeddings = sentence_embeddings[start:start + window_size]\n",
    "            combined_window_embedding = window_embeddings.mean(dim=0, keepdim=True)\n",
    "\n",
    "            # Calculate cosine similarity between the current window and known signature embeddings\n",
    "            cosine_scores = util.cos_sim(combined_window_embedding, signature_embeddings)\n",
    "            max_score = max(cosine_scores[0])\n",
    "\n",
    "            if max_score > threshold:\n",
    "                print(f\"Match: {max_score:.4f}\")\n",
    "               # Remove all lines after the match\n",
    "                remaining_text = \"\\n\".join(lines[:start])\n",
    "                return remaining_text\n",
    "    # No match found\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection: \n",
    "\n",
    "1. I made some progress on improving the diff, and exploring alternative methods. I found that the diff library they were using was comparing sentences, so by changing this to match every single character the similarity score increased - which was, *technically*, the objective. \n",
    "\n",
    "2. I learned a lot - becoming more familiar with python, the pitfalls of data analysis - it's easy to get lost in the minutia.  \n",
    "\n",
    "3. Something that interested me, but I was not able to explore, was using the diff scores to improve the generation process. We have the data for each diff - it would be interesting to try collating these and sorting them by similar corrections to produce recommendations or common problems. You could imagine this being done automatically and generating a list of top 'x' priorities to make the generation more accurate, along with the % improvement each would bring. \n",
    "\n",
    "    > Whenever I can, I like to automate. This drive and desire to explore new ideas is something that I look forward to bringing to a smaller company, with more cross-functional opportunities\n",
    "\n",
    "\n",
    "\n",
    "The lead of the project remarked:\n",
    "\"Your curiosity, ability to learn new concepts, and come up with out-of-the-box ideas are impressive.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speech to LLM App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think well when walking, and I would like to be able to brainstorm ideas while in that flow state. Going back for my computer or pulling out my phone often interrupts what I'm thinking. It would be nice to have voice transcribed notes.\n",
    "\n",
    "Additionally, I find that a conversational dialog is conducive to developing better ideas. I worked on this for a couple of weeks in July while I had some free time. \n",
    "\n",
    "The idea is simple ->\n",
    "1. A basic iOS app that sends audio to a Node server on my computer. \n",
    "\n",
    "CODE\n",
    "\n",
    "2. The Node server transcribes the audio and listens for a wake word. \n",
    "\n",
    "CODE\n",
    "\n",
    "\n",
    "3. Upon this wake word a call to ChatGPT is triggered with the past `x` amount of context, and a response is generated.\n",
    "\n",
    "CODE\n",
    "\n",
    "\n",
    "4. The response is turned into audio, and sent to the app. The conversation transcription & response are stored as notes.\n",
    "\n",
    "CODE\n",
    "\n",
    "\n",
    "Some things I considered doing but didn’t get around to were:\n",
    "1. Adding keywords that could create new notes, or give the context of the previous x lines of notes to ChatGPT, along with a command to format it in some way.\n",
    "2. More generally, giving more options to ChatGPT - e.g. searching, or custom function calling. \n",
    "\n",
    "I stopped this project because I wasn't convinced I was actually getting more benefit out of it than going on a walk to clear my mind and then writing on my computer. \n",
    "\n",
    "I still think that a `perfect` AI assistant with transcription and note taking and function calling, *AND* a LLM re-writing the notes for clarity/ future reference, or some sort of mental model, would be *decently* useful.\n",
    "\n",
    "Unforunately this would require:\n",
    "1. Much more development & edge case debugging\n",
    "2. A period of adaptation for me => I found that I mumble a lot, and directly turning my thoughts into words *coherently enough* is harder than I thought. \n",
    "\n",
    "And I was not ready to commit to both of these. \n",
    "\n",
    "Takeaways:\n",
    "1. I do not enjoy Swift - However, I was able to hack together an app using a lot of ChatGPT and debugging. \n",
    "    \n",
    "    > Tech stacks are just *tools* that allow you to do *things*. Doing *things* is what’s important. \n",
    "2. I didn’t have a measurable goal for this prototype stage - \n",
    "    1. As I mentioned above, the stage I reached was not super helpful to me, which meant I wasn't able to validate this project super well. \n",
    "    2. It’s important to build in measurable outcomes to prototype stages - how can I validate if this idea is *worth continuing*? What value can I derive from *each stage*\n",
    "\n",
    "Overall, I learned a bit about calling LLMs, voice recording, audio processing, node Streams, and node servers.\n",
    "got a good benchmark for how well I could do something technically unfamiliar with ChatGPT. \n",
    "\n",
    "Chips => LLMs, Node, Streams, Swift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "//helpers/rev-streaming.ts\n",
    "\n",
    "// RevClient, revAiStream, and bufferStream are global variables passed in by reference, and modified in this function\n",
    "export function startStreamingSession (revClient: RevAiStreamingClient | null, revAiStream: Duplex | null, bufferStream: PassThrough | null) {\n",
    "    revClient = new RevAiStreamingClient(REV_AI_API_KEY, audioConfig)\n",
    "    revAiStream = revClient.start()\n",
    "  \n",
    "    // Listen once for initialization\n",
    "    let connectedPromise: Promise<{ connected: boolean; error?: string }> =\n",
    "      new Promise((resolve, _) => {\n",
    "        revClient!.once('connectFailed', error => {\n",
    "          resolve({ connected: false, error })\n",
    "        })\n",
    "  \n",
    "        revClient!.once('connect', connectionMessage => {\n",
    "          console.log(\n",
    "            `Connected with message: ${JSON.stringify(connectionMessage)}`\n",
    "          )\n",
    "          resolve({ connected: true })\n",
    "        })\n",
    "  \n",
    "        setTimeout(() => {\n",
    "          resolve({\n",
    "            connected: false,\n",
    "            error: 'Took longer than 5 seconds to initiate'\n",
    "          })\n",
    "        }, 5000)\n",
    "      })\n",
    "  \n",
    "    //Add listeners to audio stream\n",
    "    audioProcessing(revAiStream)\n",
    "  \n",
    "    // Buffer stream to handle incoming PCM data\n",
    "    bufferStream = new PassThrough()\n",
    "  \n",
    "    // Pipe the duplex stream to our revAiStream so it can be accessed elsewhere\n",
    "    bufferStream.pipe(revAiStream)\n",
    "  \n",
    "    return connectedPromise\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "//text_processing.ts\n",
    "\n",
    "export function processTextListener (\n",
    "  transcriptionDataObject: TranscriptionDataObject,\n",
    "  revAiStream: Duplex,\n",
    "  outputPaths: OutputPaths\n",
    ") {\n",
    "  // Process events received from Rev.ai transcript (response) stream. rev.ai returns 'partial' and 'final' chunks\n",
    "  revAiStream.on('data', data => {\n",
    "    console.log('*')\n",
    "    if (data.type === 'partial') {\n",
    "      const newWord: string = data.elements[data.elements.length - 1]\n",
    "        .value as string\n",
    "      transcriptionDataObject.partialData += newWord + ' '\n",
    "    } else if (data.type === 'final') {\n",
    "      let sentence = ''\n",
    "      for (const element of data.elements) {\n",
    "        sentence +=\n",
    "          element.value +\n",
    "          (element.type === 'punct' && element.value === '.' ? '\\n' : ' ')\n",
    "      }\n",
    "      transcriptionDataObject.finalData += sentence\n",
    "      transcriptionDataObject.partialData = '' \n",
    "    }\n",
    "\n",
    "    // Call format - I was experimenting w/ having ChatGPT format the notes to fix transciption errors, of which there were many\n",
    "    if (transcriptionDataObject.finalData.length >= 300) {\n",
    "      formatDocument(transcriptionDataObject.finalData, outputPaths)\n",
    "      transcriptionDataObject.finalData = '' \n",
    "    }\n",
    "  })\n",
    "\n",
    "  revAiStream.on('close', (err: any) => {\n",
    "   \n",
    "  })\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "//index.ts\n",
    "\n",
    "/** Initiate a new recording stream.\n",
    " * Calls startStreamingSession & processTextListener\n",
    " */\n",
    "app.get('/startSession', async (_, res) => {\n",
    "    const response = await startStreamingSession(revClient, revAiStream, bufferStream)\n",
    "    if (response.connected) {\n",
    "      outputPaths = setupFolders()\n",
    "      \n",
    "      processTextListener(\n",
    "        transcriptionDataObject,\n",
    "        revAiStream as Duplex,\n",
    "        outputPaths\n",
    "      )\n",
    "      res.sendStatus(200)\n",
    "    } else {\n",
    "      res.status(400).send(`Error starting session: ${response.error}`)\n",
    "    }\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// call to gpt w/ context - node stream\n",
    "\n",
    "//.. part of wake function\n",
    "    // Get the formatted notes. Add in the transciptionDataObject. Send this to the gpt api\n",
    "    let formattedFile = ''\n",
    "    const ffPath = `${outputPaths.generated}/formatted_note.md`\n",
    "    if (fs.existsSync(ffPath)) {\n",
    "      formattedFile = fs.readFileSync(ffPath, 'utf-8')\n",
    "    }\n",
    "    const fChunk =\n",
    "      formattedFile.length > 4000 ? formattedFile.slice(-4000) : formattedFile\n",
    "    const extraData =\n",
    "      transcriptionDataObject.finalData + transcriptionDataObject.partialData\n",
    "    formatDocument(extraData, outputPaths)\n",
    "    const chatResponse = await getAnswerStreaming(fChunk + extraData)\n",
    "\n",
    "    // Convert gpt response stream to a text stream\n",
    "    const textStream = new Transform({\n",
    "      writableObjectMode: true,\n",
    "      transform (chunk: ChatCompletionChunk, encoding, callback) {\n",
    "        const finishReason = chunk.choices[0].finish_reason\n",
    "        if (finishReason) {\n",
    "          if (finishReason !== 'stop') console.log(finishReason)\n",
    "          // End stream\n",
    "          this.push(null)\n",
    "          return callback()\n",
    "        }\n",
    "        this.push(chunk.choices[0].delta.content ?? 'ERR')\n",
    "        callback()\n",
    "      }\n",
    "    })\n",
    "    chatResponse?.pipe(textStream)\n",
    "    // Text to speech (shown in next code chunk)\n",
    "    const audioStream = textStreamToAudioStream(textStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "/** Create a passthrough, to pipe the audioStream through it. \n",
    " * As text is recieved (in chunks of 100 or line end), run it through the TTS.\n",
    " * Write returned audio to the passthrough. When that audio ends, resume textStream\n",
    " * When the text stream ends, handle the last bit\n",
    " * Returns the readable passThroughStream\n",
    "*/\n",
    "export function textStreamToAudioStream (textStream: Readable): Readable {\n",
    "    const passThroughStream = new PassThrough()\n",
    "    let accumulatedText = ''\n",
    "  \n",
    "    textStream.on('data', async chunk => {\n",
    "      accumulatedText += chunk.toString()\n",
    "      if (accumulatedText.length > 100 || accumulatedText.includes('\\n')) {\n",
    "        textStream.pause() // Pause the text stream to wait for the current chunk to be processed\n",
    "        try {\n",
    "          const audioStream = (await convertTextToSpeech(\n",
    "            accumulatedText\n",
    "          )) as Readable\n",
    "          audioStream.pipe(passThroughStream, { end: false })\n",
    "          audioStream.on('end', () => {\n",
    "            textStream.resume() // Resume the text stream after the chunk is processed\n",
    "          })\n",
    "        } catch (err) {\n",
    "          passThroughStream.emit('error', err)\n",
    "        }\n",
    "        accumulatedText = ''\n",
    "      }\n",
    "    })\n",
    "  \n",
    "    textStream.on('end', async () => {\n",
    "      if (accumulatedText) {\n",
    "        try {\n",
    "          const audioStream = (await convertTextToSpeech(\n",
    "            accumulatedText\n",
    "          )) as Readable\n",
    "          audioStream.pipe(passThroughStream, { end: false })\n",
    "          audioStream.on('end', () => {\n",
    "            passThroughStream.end() // End when the last chunk is processed\n",
    "          })\n",
    "        } catch (err) {\n",
    "          passThroughStream.emit('error', err)\n",
    "        }\n",
    "      } else {\n",
    "        passThroughStream.end() // End if there's no remaining text\n",
    "      }\n",
    "    })\n",
    "  \n",
    "    textStream.on('error', err => {\n",
    "      passThroughStream.emit('error', err)\n",
    "    })\n",
    "  \n",
    "    return passThroughStream\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cisco Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I competed in an internal Cisco hackathon to explore how we could improve  customer experience use cases with AI. \n",
    "\n",
    "My team won 1st place out of over 20 teams!\n",
    "\n",
    "The compeotition took place over the span of two weeks. It was hosted by a different organization in Cisco, so it was different products and a different setting than I'm used to. Luckily my team had some experience we were able to draw on.\n",
    "\n",
    "I had a lot of fun meeting people, collaborating, and creating our project. \n",
    "\n",
    "\n",
    "I played a few key roles on the team: \n",
    "1. Collaborating closely with our team lead to hash out his ideas and customer knowledge, and come up with a winning idea. He has a lot of knowledge around the specific field; *together* we were able to turn this into an implementation. \n",
    "2. Executing on this; I quickly learned how to build, and built, a smart AI assistant using Streamlit and a RAG system with FAISS and LangChain.\n",
    "\n",
    "This showcases my holistic skill set. <span id=\"text-accent\">I’m at my best when I have the ability to work closely with the product, with stakeholders, and with the implementation itself!</span>\n",
    "\n",
    "My team lead graciously said this about me:\n",
    "\n",
    "> \"Daniel continuously demonstrated his ability to navigate high-level business strategy and translate that into ideas, concepts, code, and connections that support the achievement of those goals.\"\n",
    "\n",
    "** add some more about this showing my ideation, prototyping, and ability to see the big picture, while also coding it! **\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
