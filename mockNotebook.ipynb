{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this to write out my resume, then copying it. \n",
    "\n",
    "Would be a fun project to do this automatically, now that I have all the styling etc. for it :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daniel McCann-Sayles' Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"display: flex; justify-content: space-between; right-margin: 50px \">\n",
    "  <span>Cisco</span>\n",
    "  <span \">thing2</span>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary :p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cisco - Intern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kid Fight Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My friend Devin is interested in game design. I was talking with him about how having ‘real AI’ in games is going to allow awesome interactive stuff. I thought it would be fun to see what could be done creatively with AI\n",
    "\n",
    "As a kid I used to have arguments with friends where we would one up the other. I would have a laser, he would have a laser deflecting mirror, I would have gone back in time and weakened his mirror so it cracked, etc. \n",
    "\n",
    "I thought it would be fun to approximate some of this back and forth creativity - in a table top manner, like Pokemon.\n",
    "There were two distinct phases to this project.\n",
    "\n",
    "I'll talk a bit about each, but only show code for the second. You can check out the repository here *TODO*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First pass / Pygame era:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first started by finding a pygame game implementing chess. It made sense to just start with that chessboard. Unforunately this introduced a pattern of coupled game display and logic that lasted for a while. \n",
    "\n",
    "This was pretty fun. Lots of learning how to use pygame. Learned about pygame events to allow certain things to happen in time w/ clock. For calling chatgpt we did this asynchronously. To do this I made a new thread, which is debatably the right answer, but it works fine. We started implementing some path finding and logic for the little game pieces. \n",
    "\n",
    "I learned a fair amount about OOP as well, something I'm not super familiar with. Pygame is cool, but it also took a fair amount of boilerplate to make something simple. Though maybe this is just all game development -> not my field :).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second pass / Simulation era:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I ended up being busy for a bit, but when I came back I had a fresh perspective. I realized that this would never be fun/useful unless two people could type, which necessitates multiplayer. So I set up a simple network protocol using the socket library. \n",
    "\n",
    "This also led me to decouple the display logic and the game logic, and cleared up a lot. Instead of having all these pygame classes that handled their state and display, I have all that logic in python classes on the server. I communicate the board state through a json object, and have pygame classes on the frontend/client. You can see below the seperation of logic - the client cares about looks, and the server cares about functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Client implementation of Board\n",
    "class Board:\n",
    "    def __init__(self, size: int, x_offset, y_offset, rows: int):\n",
    "        self.width = size\n",
    "        self.height = size\n",
    "        self.x_offset = x_offset\n",
    "        self.y_offset = y_offset\n",
    "        self.square_size = size // rows\n",
    "        self.rows = rows\n",
    "        self.squares: list[Square] = self.generate_squares()\n",
    "\n",
    "    def update_board(self, board_list: list[str | PieceData]):\n",
    "        \"\"\"Update board from json data. Clear everything on squares, then re-add\"\"\"\n",
    "        if len(self.squares) != len(board_list):\n",
    "            print(\n",
    "                \"This should not happen.. mismatch btwn server and client \",\n",
    "                len(self.squares),\n",
    "                len(board_list),\n",
    "            )\n",
    "\n",
    "        for data, square in zip(board_list, self.squares):\n",
    "            square.occupying_piece = None\n",
    "            square.highlight = False\n",
    "\n",
    "            if is_piece_data(data):\n",
    "                piece = Piece(self.square_size, data)\n",
    "                square.occupying_piece = piece\n",
    "\n",
    "            elif data == \"H\":\n",
    "                square.highlight = True\n",
    "\n",
    "    def draw(self, display):\n",
    "        for square in self.squares:\n",
    "            square.draw(display)\n",
    "            \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Server implementation of Board\n",
    "class Board:\n",
    "    def __init__(self, rows: int):\n",
    "        self.rows = rows\n",
    "        self.squares: list[list[Square]] = self.generate_squares()\n",
    "        self.bases = self.make_bases()\n",
    "        self.characters: dict[str, list[Character]] = {\"black\": [], \"white\": []}\n",
    "\n",
    "    def to_json(self):\n",
    "        \"\"\"Got through the squares and serialize each of them and return it. Returns a flattened list\"\"\"\n",
    "        flat_squares = [square for columns in self.squares for square in columns]\n",
    "        json_list = [square.to_json() for square in flat_squares]\n",
    "\n",
    "        return json_list\n",
    "\n",
    "    def generate_squares(self):\n",
    "        output: list[list[Square]] = []\n",
    "        for row in range(self.rows):\n",
    "            inner_list = []\n",
    "            for column in range(self.rows):\n",
    "                inner_list.append(Square(row, column))\n",
    "\n",
    "            output.append(inner_list)\n",
    "        return output\n",
    "\n",
    "    def add_piece_safe(self, pos: tuple[int], piece: DefaultPiece):\n",
    "        \"\"\"Add the given piece to the given position on the board. Fails quietly w/ console log. Sets the pieces position to square position\"\"\"\n",
    "        square = self.get_pos(pos)\n",
    "        if not square:\n",
    "            print(\"invalid square\", pos)\n",
    "            return\n",
    "        if self.check_if_occupied(square):\n",
    "            print(\"already occupied\")\n",
    "            return\n",
    "\n",
    "        square.occupying_piece = piece\n",
    "        piece.set_pos((square.row, square.column))\n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They communicate through a simple network protocol, using sockets.\n",
    "\n",
    "`mserver.py` hosts a server which starts threaded processes as clients join. `mclient.py` initiates a new pygame (`SrverGame.py`) process that uses the Network (`network.py`) class to communicate with the server. \n",
    "\n",
    "Below are snippets from each to hopefully convey their main functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mserver\n",
    "import socket\n",
    "from _thread import start_new_thread\n",
    "from client.helpers import get_mock_board\n",
    "import json\n",
    "from server.ServerGame import ServerGame\n",
    "from address import SERVER_ADDRESS\n",
    "\n",
    "# Class to handle game logic. \n",
    "server_game = ServerGame()\n",
    "\n",
    "# Global to communicate whether players are here or not\n",
    "players = {\"black\": False, \"white\": False}\n",
    "\n",
    "def threaded_client(conn: socket.socket, color: str):\n",
    "    # Start connection by sending initial obj\n",
    "    initial = json.dumps(\n",
    "        {\"loading\": False, \"board\": server_game.board.to_json()}\n",
    "    ).encode()\n",
    "    conn.send(initial)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "           # Recieve Data ..\n",
    "\n",
    "            # If we get data saying we want a new character\n",
    "            if \"description\" in json_data:\n",
    "                server_game.create_character(color, json_data[\"description\"])\n",
    "                print(\"recieved description \", json_data[\"description\"])\n",
    "\n",
    "            # Call this every iteration. handles game logic & and other stuff\n",
    "            board_json, loading = server_game.gameloop(color)\n",
    "\n",
    "            # Return \n",
    "            conn.sendall(json.dumps({\"loading\": loading, \"board\": board_json}).encode())\n",
    "        except Exception as e:\n",
    "           break\n",
    "\n",
    "    print(\"Lost connection to \", color)\n",
    "    players[color] = False\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "\"\"\" Handle new connections\"\"\"\n",
    "while True:\n",
    "    p1 = players[\"black\"]\n",
    "    p2 = players[\"white\"]\n",
    "\n",
    "    conn, addr = s.accept()\n",
    "\n",
    "    # Add players .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mclient => ServerGame.py\n",
    "import time\n",
    "import threading\n",
    "from llm.call_gpt import generate_character_stats_multiplayer\n",
    "from server.classes.Board import Board\n",
    "from server.classes.Character import Character\n",
    "\n",
    "\n",
    "class Clock:\n",
    "    def __init__(self, interval, stopped=False):\n",
    "        \"\"\"Interval in seconds\"\"\"\n",
    "        self.interval = interval\n",
    "        self.last_time = time.time()\n",
    "        self.stopped = stopped\n",
    "\n",
    "    def check(self):\n",
    "        \"\"\"Returns true if interval has passed\"\"\"\n",
    "\n",
    "    ...\n",
    "\n",
    "\n",
    "class ServerGame:\n",
    "    def __init__(self):\n",
    "        self.main_clock = Clock(1)\n",
    "        self.sub_clock = Clock(1, stopped=True)\n",
    "\n",
    "        # Board object for representation.\n",
    "        self.board = Board(10)\n",
    "\n",
    "        # Lock prevents main check from running twice (called in two threads)\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def callback(self, response, color):\n",
    "        \"\"\"Callback is a method to have access to self - called in thread to get async GPT\"\"\"\n",
    "        self.character_to_add[color] = Character(\n",
    "            (0, 0),\n",
    "            color=color,\n",
    "            board=self.board,\n",
    "            attack_dmg=response[\"AD\"],\n",
    "            hp=response[\"HP\"],\n",
    "            move_distance=response[\"MD\"],\n",
    "        )\n",
    "\n",
    "    def create_character(self, color, description: str):\n",
    "        \"Call gpt to create a character. Currently making a new thread to do so. daemon means it will be terminated\"\n",
    "        if self.loading_character[color]:\n",
    "            return\n",
    "\n",
    "        self.loading_character[color] = True\n",
    "        description = description or \"bland default character\"\n",
    "\n",
    "        threading.Thread(\n",
    "            target=generate_character_stats_multiplayer,\n",
    "            args=(description, color, self.callback),\n",
    "            daemon=True,\n",
    "        ).start()\n",
    "\n",
    "   \n",
    "\n",
    "    def gameloop(self, color):\n",
    "        \"\"\"Runs constantly in while loop\"\"\"\n",
    "        if self.main_clock.check():\n",
    "            # Do stuff..\n",
    "\n",
    "        if self.sub_clock.check():\n",
    "            self.move_character()\n",
    "\n",
    "        # Check if we need to add a character\n",
    "        character = self.character_to_add[color]\n",
    "        if character:\n",
    "            # Add character..\n",
    "\n",
    "        # Return current board\n",
    "        return self.board.to_json(), self.loading_character[color]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original idea of this game was to explore the added creativity an LLM could bring. In this case, the idea was to have an impartial judge add or subtract to bonuses to characters depending on the existing characters on the board.\n",
    "\n",
    "A next step was to add behavior, and then to add special moves, but we did not get around to this.\n",
    "Instead we were pretty sidetracked by building out the main game engine functionality.\n",
    "Anyways heres the general prompt/schema we hoped to follow.\n",
    "\n",
    "Of some note is the Strength section. We wanted to allow the LLM judge to give bonuses to characters against other characters specifically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARACTER_PROMPT = \"\"\"\n",
    "You are a character creator. Your job is to determine the stats of a character based on these two things:\n",
    "1. A list of existing characters on the board (a list of json objects, of the format {id: <int>, description: <str>}) \n",
    "2. A description of the character passed in (a string)\n",
    "\n",
    "These stats are as follows:\n",
    "1. HP - Health points. This is not to exceed 10, or go below 1. Some examples for reference:\n",
    "    A knight in armor might have a high HP, like 6. An archer might have a low HP, like 3. A mouse might have a tiny HP, like 1.\n",
    "\n",
    "    ...\n",
    "\"\"\"\n",
    "\n",
    "API_SCHEMA = {\n",
    "    \"name\": \"character_stats\",\n",
    "    \"strict\": True,\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"HP\": {\"type\": \"integer\"},\n",
    "            \"AD\": {\"type\": \"integer\"},\n",
    "            \"MD\": {\"type\": \"integer\"},\n",
    "            \"Behavior\": {\"type\": \"integer\"},\n",
    "            \"Strength\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"against_id\": {\"type\": [\"integer\", \"null\"]},\n",
    "                    \"modifier\": {\"type\": [\"integer\", \"null\"]},\n",
    "                },\n",
    "                \"required\": [\"against_id\", \"modifier\"],\n",
    "                \"additionalProperties\": False,\n",
    "            },\n",
    "        ...\n",
    "}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection:\n",
    "\n",
    "1. Learned more about async and anyio, network protocol, and OOP w/ Python\n",
    "2. It's important to always decouple logic & display - it would've been much easier to refactor for the simulation if I had followed this practice from the beginning\n",
    "\n",
    "Next steps:\n",
    "\n",
    "In hindsight, we tried to tackle two things at the same time, and it would have been better to only tackle one. Either a =>\n",
    "1. creative response game using AI to evaluate that creativity\n",
    "2. simulator/board game, with physical pieces and a map\n",
    "\n",
    "I’d actually like to make the first one right now - instead of a physical board, using some sort of card game, Magic-like system would be much better for a prototype of on the fly, AI-judged creativity. I have some other things to do first though ..\n",
    "\n",
    "This leads to one final lesson learned, which is common in many of my projects..\n",
    "> simplify prototypes & identify measurable objectives! BEFORE starting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MACM exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[test](google.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During some downtime at work, I explored a Cisco research project focused on agentic AI workflows to improve performance on math problem-solving benchmarks. \n",
    "\n",
    "I optimized the code base, explored asynchronous alternatives, and reduced costs while increasing speed. I  also explored alternative methods for connecting LLM calls and evaluated their effectiveness in problem-solving.\n",
    "\n",
    "This experience sharpened my skills in prompt engineering, OpenAI APIs, and agentic workflows, which I plan to apply to future AI applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to explore.  I started with this paper as an existing benchmark. It presents essentially a chain of LLM calls with to improve the performance. Their methodology essentially involves \n",
    "\n",
    "The code started getting a bit messy, and I got lost in tterms of metrics.\n",
    "\n",
    "Here is a rough timeline of what I attempted\n",
    "\n",
    "1. First, I looked at the existing MACM thing. They split things up into three different roles, determined ONLY by giving different prompts. \n",
    "2. I rewrote a lot of their stuff to be cleaner. I added async. I cleaned up prompts, and I switched the whole thing to a cheaper form of GPT so I could run multiple times.\n",
    "\n",
    "Then I started tweaking things. Notably, what I did not and should have done, was re-do a benchmark run with this cleaned up & quicker/cheaper setup. \n",
    "\n",
    "The first thing I added was a re-assesment of steps. I noticed that the generated steps were not \n",
    "\n",
    "\n",
    "1. The first stage just involved switching everything to a simpler form of gpt, adding async, and cleaning up the files, prompts, and organization.\n",
    "\n",
    "2. Next I started trying different methods. The goal is to improve how a series of GPT calls does on the MATH dataset. There were certain patterns I wanted to try. \n",
    "\n",
    "\n",
    "Reflections: \n",
    "\n",
    "1. This endeavor was not entirely fruitful. One big challenge is benchmarking. Every call to GPT has some element of randomness in its return. When adding multiple together, it gets very complex very quickly. Even one call can be hard since there’s no function or obvious way to calculate randomness (that I know of). It’s also kind of a nebulous problem.\n",
    "\n",
    "2. Another is how AI models are easily influenced. I imagine this is due to training that makes them agreeable to users, but when given a false idea (let’s say call #1 returned an incorrect assumption), it seems that call #2 often will not question that idea. \n",
    "\n",
    "Taking this idea a bit further, each call seems to perform worse and worse. I’m not sure why this is, though it would be interesting to read more about it. It seems impossible to do error correcting *completely* with LLM’s right now. \n",
    "\n",
    "In the end I did not achieve much noteworthy. But I understand a few things better now:\n",
    "1. python codebases\n",
    "2. python asynchronous libraries - I used anyio w/ the default configuration of asyncio backend. I like anyio.\n",
    "3. The complexities that arise in multi-LLM interactions\n",
    "4. The overhead that comes with trying to improve chained LLM performance - there are so many variables to adjust - models, prompts, chaining order, and much more. And it's not a consistent result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also had some more abstract rambling:\n",
    "\n",
    "I thought a lot about how we think, and about hiearchial forms of thought. For instance, starting at small provable concepts, using those to create higher level concepts, and so on. This is (very simplified) how we do things as humans. If I look at a complex belief I have, it’s predicated on lower and lower level ideas that have been lodged in my brain. It can be hard to change someones mind because even when a low level fact is changed, humans don't always update the higher order concepts that rest on it, or even think about that process. \n",
    "\n",
    "This leads me to something that I’ve been thinking about for a while, a way of creating a living essay. It would be cool to write a persuasive essay in a way that drew directly from the facts in it, so that when things were changed, everything downstream changed too. \n",
    "\n",
    "Of course, a lot of ideas are nebulous, but if you could lay out some amount of priors and assign some weight to each that showed how much it affected a higher level concept, then a reader could actually interact with your argument and see it change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AI Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** There are (at least) two obvious things wrong with this graph - one is that the dots overlap, meaning the last applied (gpt diff) is overrepresented. The other is that its not super helpful, except for a preliminary visualization. If I was still working on this, the next thing I would do would be sorting/clustering these data points based on how they do - e.g. these do well w/ the gpt diff, these do well with the semantic diff. \n",
    "\n",
    "\n",
    "This visualization would look cool, but would only be marginally more helpful, since it would end up with me diving back in to looking data point by data point. Ideally would be to find some way to extract similarities in the text itself, to avoid looking through each one myself. This idea of general pattern recognition reminds me of visual nn and makes me think I could train a simple model to predict how well a sample text would do on the different diff methods, and then extract the features it found. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had the opportunity to work on an internal Cisco project exploring how well an LLM email generation process was doing. A system would generate emails which are then looked over by users, optionally changed, and then sent. As a metric of how well the process was doing, the stakeholders were currently calculating a diff between the sent and generated email. \n",
    "\n",
    "I was tasked with exploring this diff, and seeing how it could be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important lesson I learned is that ‘data analysis’ is a surprisingly difficult thing to do. This motif appears later in my AI Math solving exploration as well. There are so many different ways of calculating metrics, and so many different metrics. \n",
    "\n",
    "Crucially, these metrics also depend on the audience and your intentions - the metrics you would show to an executive are completely different than those you might show to a technical stakeholder, and again different than those a customer might care about.\n",
    "\n",
    "It's fascinating. You want to create some sort of visualization out of your data that your audience can easily read. This is something I find really interesting, especially when reading AI papers. \n",
    "\n",
    "To explore the diff, I created multiple Jupyter notebooks and spent a few weeks immersing myself in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. I poured over hundreds of examples, explored different ways of both sorting & filtering, and visualizing the differences  to get impactful metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a toggleable output\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def toggleable_output(text, identifier):\n",
    "    html = f\"\"\"\n",
    "    <button onclick=\"var x = document.getElementById('toggleText_{identifier}'); x.style.display = x.style.display === 'none' ? 'block' : 'none';\">\n",
    "        Toggle {identifier}\n",
    "    </button>\n",
    "    <div id=\"toggleText_{identifier}\" style=\"display:none;\">\n",
    "        {text}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create html object to show diff visually\n",
    "def show_visual_diff_difflib(\n",
    "    old_string: str, new_string: str, wrap_column=60\n",
    "):\n",
    "    diff_html = difflib.HtmlDiff(wrapcolumn=wrap_column).make_file(old_string.split(), new_string.split(),)\n",
    "\n",
    "    # Remove legend & links\n",
    "    diff_html = re.sub(r'<table class=\"diff\" summary=\"Legends\".*?</table>', \"\", diff_html, flags=re.DOTALL)\n",
    "    diff_html = re.sub(r'</td>\\s*<td>\\s*<table border=\"\" summary=\"Links\".*?</table>\\s*</td>\\s*</tr>', \"\", diff_html, flags=re.DOTALL)\n",
    "\n",
    "    return '<style>.diff_add, .diff_chg, .diff_sub {color: black;}</style>' + diff_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow down to useful data to check out. When checking how a process affected a diff I couldn't just plot everything. It becomes a mess, as shown by the plot above\n",
    "improved_diffs = []\n",
    "for row in items: # Items is a df of calculated diff scores and text\n",
    "    new_diff = row[\"New_diff\"]\n",
    "    diff = row[\"Diff\"]\n",
    "    if new_diff - diff > .2: # I played around with this number. 0.2 usually was a good heuristic of narrowing down the data but not nitpicking\n",
    "        improved_diffs.append(row)\n",
    "\n",
    "print(f\"rows with improvements > .2: {len(improved_diffs)}, out of {len(items)}\")\n",
    "\n",
    "# For item in improved_diffs\n",
    "# Using the difflib html object + toggling to make the visualization process easier\n",
    "original_diff_text = f\"Original diff:\\n{item['Diff']}\\n\" + show_visual_diff_difflib(item[\"Generated\"], item[\"Sent\"])\n",
    "toggleable_output(original_diff_text, \"original_diff\")\n",
    "\n",
    "new_diff_text = f\" diff:\\n{item['new_diff']}\\n\" + show_visual_diff_difflib(item[\"new_diff_generated\"], item[\"new_diff_sent\"])\n",
    "toggleable_output(new_diff_text, \"new_diff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. I tried multiple methods for cleaning the data before taking the diff, including using ChatGPT and semantic similarity matching, to get a score that reflected *more accurately* how good the generation was. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the original text split into main part and trailing addition\n",
    "def check_if_trailing_addition(sent: str, generated: str):\n",
    "    testing_diff = list(\n",
    "        difflib.ndiff(\n",
    "            generated.strip().splitlines(),\n",
    "            sent.strip().splitlines(),\n",
    "        )\n",
    "    )\n",
    "    last_section_length = 0\n",
    "\n",
    "    # Find the last section of completely new lines\n",
    "    for i, line in enumerate(reversed(testing_diff)):\n",
    "        if line.startswith(\"+ \"):\n",
    "            last_section_length += 1\n",
    "        elif line.startswith(\"? \"):  # difflib metadata\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if last_section_length > 0:\n",
    "        # Split into main & trailing addition\n",
    "        sent_lines = sent.strip().splitlines()\n",
    "        main_part = \"\\n\".join(sent_lines[:-last_section_length])\n",
    "        trailing_addition = \"\\n\".join(sent_lines[-last_section_length:])\n",
    "        return trailing_addition, main_part\n",
    "\n",
    "    return False, sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to detect common signature chunks ----------\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "signature_embeddings = model.encode(known_signature_chunks, convert_to_tensor=True)\n",
    "\n",
    "# Pre-vectorize searchable text, broken down by sentence level\n",
    "def vectorize_text_by_sentences(text: str, model: SentenceTransformer):\n",
    "    sentences = text.splitlines()\n",
    "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    return sentences, sentence_embeddings\n",
    "\n",
    "def remove_signature_chunks(trailing_addition: str, signature_embeddings, model: SentenceTransformer, threshold=0.6):\n",
    "    lines, sentence_embeddings = vectorize_text_by_sentences(trailing_addition, model)\n",
    "\n",
    "     # Sliding window - to match and remove signatures\n",
    "    for start in range(len(lines)):\n",
    "        for window_size in range(1, len(lines) - start + 1):\n",
    "            # Extract the embeddings for the current window of sentences\n",
    "            window_embeddings = sentence_embeddings[start:start + window_size]\n",
    "            combined_window_embedding = window_embeddings.mean(dim=0, keepdim=True)\n",
    "\n",
    "            # Calculate cosine similarity between the current window and known signature embeddings\n",
    "            cosine_scores = util.cos_sim(combined_window_embedding, signature_embeddings)\n",
    "            max_score = max(cosine_scores[0])\n",
    "\n",
    "            if max_score > threshold:\n",
    "                print(f\"Match: {max_score:.4f}\")\n",
    "               # Remove all lines after the match\n",
    "                remaining_text = \"\\n\".join(lines[:start])\n",
    "                return remaining_text\n",
    "    # No match found\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection: \n",
    "\n",
    "1. I made some progress on improving the diff, and exploring alternative methods. I found that the diff library they were using was comparing sentences, so by changing this to match every single character the similarity score increased - which was, *technically*, the objective. \n",
    "\n",
    "2. I learned a lot - becoming more familiar with python, the pitfalls of data analysis - it's easy to get lost in the minutia.  \n",
    "\n",
    "3. Something that interested me, but I was not able to explore, was using the diff scores to improve the generation process. We have the data for each diff - it would be interesting to try collating these and sorting them by similar corrections to produce recommendations or common problems. You could imagine this being done automatically and generating a list of top 'x' priorities to make the generation more accurate, along with the % improvement each would bring. \n",
    "\n",
    "    > Whenever I can, I like to automate. This drive and desire to explore new ideas is something that I look forward to bringing to a smaller company, with more cross-functional opportunities\n",
    "\n",
    "\n",
    "\n",
    "The lead of the project remarked:\n",
    "\"Your curiosity, ability to learn new concepts, and come up with out-of-the-box ideas are impressive.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speech to LLM App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think well when walking, and I would like to be able to brainstorm ideas while in that flow state. Going back for my computer or pulling out my phone often interrupts what I'm thinking. It would be nice to have voice transcribed notes.\n",
    "\n",
    "Additionally, I find that a conversational dialog is conducive to developing better ideas. I worked on this for a couple of weeks in July while I had some free time. \n",
    "\n",
    "The idea is simple ->\n",
    "1. A basic iOS app that sends audio to a Node server on my computer. \n",
    "\n",
    "CODE\n",
    "\n",
    "2. The Node server transcribes the audio and listens for a wake word. \n",
    "\n",
    "CODE\n",
    "\n",
    "\n",
    "3. Upon this wake word a call to ChatGPT is triggered with the past `x` amount of context. This \n",
    "\n",
    "CODE\n",
    "\n",
    "\n",
    "4. All the transcribed audio and responses are kept as notes.\n",
    "\n",
    "CODE\n",
    "\n",
    "\n",
    "Some things I considered doing but didn’t get around to were:\n",
    "1. Adding keywords that could create new notes, or give the context of the previous x lines of notes to ChatGPT, along with a command to format it in some way.\n",
    "2. More generally, giving more options to ChatGPT - e.g. searching, or custom function calling. \n",
    "\n",
    "I stopped this project because I wasn't convinced I was actually getting more benefit out of it than going on a walk to clear my mind and then writing on my computer. \n",
    "\n",
    "I still think that a `perfect` AI assistant with transcription and note taking and function calling, *AND* a LLM re-writing the notes for clarity/ future reference, or some sort of mental model, would be *decently* useful.\n",
    "\n",
    "Unforunately this would require:\n",
    "1. Much more development & edge case debugging\n",
    "2. A period of adaptation for me => I found that I mumble a lot, and directly turning my thoughts into words *coherently enough* is harder than I thought. \n",
    "\n",
    "And I was not ready to commit to both of these. \n",
    "\n",
    "Takeaways:\n",
    "1. I do not enjoy Swift - However, I was able to hack together an app using a lot of ChatGPT and debugging. \n",
    "    \n",
    "    > Tech stacks are just *tools* that allow you to do *things*. Doing *things* is what’s important. \n",
    "2. I didn’t have a measurable goal for this prototype stage - \n",
    "    1. As I mentioned above, the stage I reached was not super helpful to me, which meant I wasn't able to validate this project super well. \n",
    "    2. It’s important to build in measurable outcomes to prototype stages - how can I validate if this idea is *worth continuing*? What value can I derive from *each stage*\n",
    "\n",
    "Overall, I learned a bit about calling LLMs, voice recording, audio processing, node Streams, and node servers.\n",
    "got a good benchmark for how well I could do something technically unfamiliar with ChatGPT. \n",
    "\n",
    "Chips => LLMs, Node, Streams, Swift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cisco Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I competed in an internal Cisco hackathon to explore how we could improve  customer experience use cases with AI. \n",
    "\n",
    "My team won 1st place out of over 20 teams!\n",
    "\n",
    "The compeotition took place over the span of two weeks. It was hosted by a different organization in Cisco, so it was different products and a different setting than I'm used to. Luckily my team had some experience we were able to draw on.\n",
    "\n",
    "I had a lot of fun meeting people, collaborating, and creating our project. \n",
    "\n",
    "\n",
    "I played a few key roles on the team: \n",
    "1. Collaborating closely with our team lead to hash out his ideas and customer knowledge, and come up with a winning idea. He has a lot of knowledge around the specific field; *together* we were able to turn this into an implementation. \n",
    "2. Executing on this; I quickly learned how to build, and built, a smart AI assistant using Streamlit and a RAG system with FAISS and LangChain.\n",
    "\n",
    "This showcases my holistic skill set. <span id=\"text-accent\">I’m at my best when I have the ability to work closely with the product, with stakeholders, and with the implementation itself!</span>\n",
    "\n",
    "My team lead graciously said this about me:\n",
    "\n",
    "> \"Daniel continuously demonstrated his ability to navigate high-level business strategy and translate that into ideas, concepts, code, and connections that support the achievement of those goals.\"\n",
    "\n",
    "** add some more about this showing my ideation, prototyping, and ability to see the big picture, while also coding it! **\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
